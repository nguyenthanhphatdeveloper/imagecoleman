#!/usr/bin/env python3
"""
Coleman async downloader
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
- Nh·∫≠p m√£ s·∫£n ph·∫©m th·ªß c√¥ng, queue cho ƒë·∫øn khi g√µ 'yes'
- T·∫£i ·∫£nh + m√¥ t·∫£ (JP) + d·ªãch sang TI·∫æNG VI·ªÜT
"""

import asyncio
import logging
from pathlib import Path

import aiofiles
import aiohttp
from bs4 import BeautifulSoup
from tqdm.asyncio import tqdm_asyncio
from deep_translator import GoogleTranslator

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ CONFIG ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
MAX_CONN = 10                  # t·ªïng socket ƒë·ªìng th·ªùi
MAX_RETRIES = 3                # s·ªë l·∫ßn th·ª≠ l·∫°i khi l·ªói
SLIDE_RANGE = range(1, 16)     # slide 1-15
SRC_LANG, DST_LANG = "ja", "vi"
translator = GoogleTranslator(source=SRC_LANG, target=DST_LANG)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ LOGGING ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
logging.basicConfig(
    level=logging.INFO,
    format="%(levelname)-8s %(message)s",
    handlers=[logging.StreamHandler(),
              logging.FileHandler("download.log", encoding="utf-8")]
)
log = logging.getLogger("coleman")


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ NETWORK HELPERS ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
async def fetch_html(product_id: str, session: aiohttp.ClientSession, 
                     retries: int = MAX_RETRIES) -> str | None:
    url = f"https://ec.coleman.co.jp/item/{product_id}.html"
    for attempt in range(retries):
        try:
            async with session.get(url) as r:
                if r.status == 200:
                    return await r.text()
                elif r.status == 404:
                    log.error("%s ‚Äì kh√¥ng t√¨m th·∫•y (404)", product_id)
                    return None
                log.warning("%s ‚Äì HTTP %s (l·∫ßn th·ª≠ %d/%d)", product_id, r.status, attempt + 1, retries)
        except asyncio.TimeoutError:
            log.warning("%s ‚Äì timeout (l·∫ßn th·ª≠ %d/%d)", product_id, attempt + 1, retries)
        except Exception as e:
            log.warning("%s ‚Äì l·ªói: %s (l·∫ßn th·ª≠ %d/%d)", product_id, e, attempt + 1, retries)
        
        if attempt < retries - 1:
            await asyncio.sleep(2 ** attempt)  # exponential backoff
    
    log.error("%s ‚Äì th·∫•t b·∫°i sau %d l·∫ßn th·ª≠", product_id, retries)
    return None


async def download_image(url: str, path: Path,
                         session: aiohttp.ClientSession,
                         product_id: str, slide: int,
                         sem: asyncio.Semaphore,
                         retries: int = MAX_RETRIES):
    # Ki·ªÉm tra file ƒë√£ t·ªìn t·∫°i v√† c√≥ k√≠ch th∆∞·ªõc h·ª£p l·ªá
    if path.exists():
        try:
            if path.stat().st_size > 0:
                return  # File ƒë√£ t·ªìn t·∫°i v√† c√≥ d·ªØ li·ªáu
        except OSError:
            pass  # File c√≥ th·ªÉ b·ªã l·ªói, t·∫£i l·∫°i
    
    async with sem:
        for attempt in range(retries):
            try:
                async with session.get(url) as r:
                    if r.status == 200:
                        data = await r.read()
                        if len(data) > 0:  # Ki·ªÉm tra d·ªØ li·ªáu kh√¥ng r·ªóng
                            async with aiofiles.open(path, "wb") as f:
                                await f.write(data)
                            return  # Th√†nh c√¥ng
                        else:
                            log.warning("%s ‚Äì slide %d: d·ªØ li·ªáu r·ªóng", product_id, slide)
                    elif r.status == 404:
                        log.warning("%s ‚Äì slide %d kh√¥ng t·ªìn t·∫°i (404)", product_id, slide)
                        return  # Kh√¥ng c·∫ßn th·ª≠ l·∫°i cho 404
                    else:
                        log.warning("%s ‚Äì slide %d HTTP %s (l·∫ßn th·ª≠ %d/%d)", 
                                  product_id, slide, r.status, attempt + 1, retries)
            except asyncio.TimeoutError:
                log.warning("%s ‚Äì slide %d timeout (l·∫ßn th·ª≠ %d/%d)", 
                          product_id, slide, attempt + 1, retries)
            except Exception as e:
                log.warning("%s ‚Äì l·ªói slide %d: %s (l·∫ßn th·ª≠ %d/%d)", 
                          product_id, slide, e, attempt + 1, retries)
            
            if attempt < retries - 1:
                await asyncio.sleep(1 * (attempt + 1))  # Linear backoff
        
        log.error("%s ‚Äì slide %d th·∫•t b·∫°i sau %d l·∫ßn th·ª≠", product_id, slide, retries)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ CORE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
async def translate_text(text: str) -> str:
    """D·ªãch text trong thread pool ƒë·ªÉ kh√¥ng block event loop"""
    loop = asyncio.get_event_loop()
    try:
        return await loop.run_in_executor(None, translator.translate, text)
    except Exception as e:
        log.error("L·ªói d·ªãch: %s", e)
        return text  # Tr·∫£ v·ªÅ text g·ªëc n·∫øu l·ªói

async def save_descriptions(soup: BeautifulSoup, out_dir: Path,
                            product_id: str):
    ul = soup.find("ul", class_="p-item_info_indt")
    if not ul:
        log.warning("%s ‚Äì kh√¥ng t√¨m <ul>", product_id)
        return
    jp_lines = [li.get_text(strip=True) for li in ul.find_all("li") if li.get_text(strip=True)]
    if not jp_lines:
        log.warning("%s ‚Äì <ul> r·ªóng", product_id)
        return

    # JP
    jp_path = out_dir / f"{product_id}.jp.txt"
    async with aiofiles.open(jp_path, "w", encoding="utf-8") as f:
        await f.write("\n".join(jp_lines))
    
    # VI - d·ªãch song song ƒë·ªÉ tƒÉng t·ªëc
    vi_tasks = [translate_text(line) for line in jp_lines]
    vi_lines = await asyncio.gather(*vi_tasks)
    
    vi_path = out_dir / f"{product_id}.vi.txt"
    async with aiofiles.open(vi_path, "w", encoding="utf-8") as f:
        await f.write("\n".join(vi_lines))
    log.info("%s ‚Äì ƒë√£ l∆∞u jp.txt & vi.txt", product_id)

async def handle_product(pid: str, session: aiohttp.ClientSession,
                         sem: asyncio.Semaphore):
    html = await fetch_html(pid, session)
    if html is None:
        return
    soup = BeautifulSoup(html, "html.parser")
    out_dir = Path(pid)
    out_dir.mkdir(exist_ok=True)

    # 1) M√¥ t·∫£
    await save_descriptions(soup, out_dir, pid)

    # 2) ·∫¢nh
    tasks = []
    for slide in SLIDE_RANGE:
        tag = soup.find(attrs={"data-slide": str(slide)})
        img_tag = tag.find("img") if tag else None
        src = img_tag["src"] if img_tag and img_tag.has_attr("src") else None
        if not src:
            log.warning("%s ‚Äì thi·∫øu slide %d", pid, slide)
            continue
        if src.startswith("//"):
            src = "https:" + src
        elif src.startswith("/"):
            src = "https://ec.coleman.co.jp" + src
        path = out_dir / f"{slide}.jpg"
        tasks.append(download_image(src, path, session, pid, slide, sem))

    if tasks:
        await tqdm_asyncio.gather(*tasks, desc=pid, unit="img")
    else:
        log.warning("%s ‚Äì kh√¥ng c√≥ ·∫£nh", pid)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ MAIN ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
async def main(product_ids: list[str]):
    sem = asyncio.Semaphore(MAX_CONN)
    # T·ªëi ∆∞u connector: tƒÉng limit_per_host ƒë·ªÉ t·∫£i ·∫£nh nhanh h∆°n
    connector = aiohttp.TCPConnector(
        limit=MAX_CONN * 2,  # TƒÉng t·ªïng connection pool
        limit_per_host=MAX_CONN,  # Cho ph√©p nhi·ªÅu connection h∆°n ƒë·∫øn c√πng host
        ttl_dns_cache=300,  # Cache DNS 5 ph√∫t
        force_close=False  # T√°i s·ª≠ d·ª•ng connection
    )

    timeout = aiohttp.ClientTimeout(total=180, connect=30)
    
    async with aiohttp.ClientSession(
        connector=connector, 
        timeout=timeout,
        headers={"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}
    ) as sess:
        tasks = [handle_product(pid, sess, sem) for pid in product_ids]
        await tqdm_asyncio.gather(*tasks, desc="T·ªïng ti·∫øn ƒë·ªô", unit="s·∫£n ph·∫©m")

if __name__ == "__main__":
    # 1) Nh·∫≠p queue
    queue: list[str] = []
    print("Nh·∫≠p m√£ s·∫£n ph·∫©m, g√µ 'yes' ƒë·ªÉ b·∫Øt ƒë·∫ßu t·∫£i:")
    while True:
        s = input("> ").strip().lower()
        if s == "yes":
            break
        elif s.isdigit():
            queue.append(s)
            print(f"‚úì ƒë√£ th√™m {s}")
        else:
            print("‚ö†Ô∏è  ch·ªâ nh·∫≠p s·ªë ho·∫∑c 'yes'.")

    if not queue:
        print("Ch∆∞a c√≥ m√£ n√†o ‚ûú tho√°t.")
    else:
        asyncio.run(main(queue))
        print("\nüéâ  Xong! Ki·ªÉm tra c√°c th∆∞ m·ª•c s·∫£n ph·∫©m v√† file download.log.")
